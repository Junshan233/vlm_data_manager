import os
import json
import shutil
import sqlite3
from datetime import datetime

import pandas as pd
import streamlit as st

# ---------------------------------------------------------------------------- #
# å¸¸é‡å®šä¹‰
# ---------------------------------------------------------------------------- #
DB_PATH = "metadata.db"            # SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„
UPLOAD_DIR = "uploads"             # æœ¬åœ°ä¸Šä¼ æ–‡ä»¶å­˜å‚¨æ ¹ç›®å½•
ITEMS_PER_PAGE = 4                  # é¢„è§ˆæ¯é¡µæ˜¾ç¤ºæ¡ç›®æ•°

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_DIR, exist_ok=True)

# ---------------------------------------------------------------------------- #
# æ•°æ®åº“åˆå§‹åŒ–ä¸è¿æ¥
# ---------------------------------------------------------------------------- #
@st.cache_resource
def get_db_connection(db_path: str = DB_PATH) -> sqlite3.Connection:
    """
    åˆå§‹åŒ–å¹¶è¿”å› SQLite æ•°æ®åº“è¿æ¥ï¼Œä½¿ç”¨ Streamlit å•ä¾‹ç¼“å­˜ä¿è¯å…¨å±€å”¯ä¸€ã€‚
    """
    conn = sqlite3.connect(db_path, check_same_thread=False)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS datasets (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            path TEXT NOT NULL,
            upload_time TEXT NOT NULL,
            tags TEXT DEFAULT '[]',
            data_type TEXT NOT NULL,
            content_path TEXT NOT NULL,
            root_path TEXT NOT NULL
        )
    """)
    conn.commit()
    return conn

conn = get_db_connection()

# ---------------------------------------------------------------------------- #
# è¾…åŠ©å‡½æ•°
# ---------------------------------------------------------------------------- #
def get_data_type(item: dict) -> str:
    """
    æ ¹æ® JSON item ä¸­çš„å­—æ®µåˆ¤æ–­æ•°æ®ç±»å‹ã€‚
    è¿”å›å€¼: 'video' | 'multi-image' | 'single-image' | 'text'
    """
    if 'video' in item and item['video']:
        return 'video'
    if 'images' in item and item['images']:
        return 'multi-image' if len(item['images']) > 1 else 'single-image'
    return 'text'

@st.cache_data
def load_all_datasets() -> list:
    """
    ä»æ•°æ®åº“è¯»å–æ‰€æœ‰æ•°æ®é›†å…ƒä¿¡æ¯ï¼Œè¿”å›åˆ—è¡¨
    """
    cursor = conn.cursor()
    cursor.execute(
        "SELECT id, name, path, upload_time, tags, data_type, content_path, root_path FROM datasets"
    )
    return cursor.fetchall()


def update_tags(dataset_id: int, tags: list) -> None:
    """
    æ›´æ–°æŒ‡å®šæ•°æ®é›†çš„æ ‡ç­¾å­—æ®µã€‚
    å‚æ•°:
      - dataset_id: æ•°æ®é›†ä¸»é”®
      - tags: æ ‡ç­¾åˆ—è¡¨ï¼ˆPython listï¼‰ï¼Œä¼šè¢«åºåˆ—åŒ–ä¸º JSON å­˜å‚¨
    """
    cursor = conn.cursor()
    cursor.execute(
        "UPDATE datasets SET tags = ? WHERE id = ?",
        (json.dumps(tags, ensure_ascii=False), dataset_id)
    )
    conn.commit()


def import_jsonl_dataset(dataset_name: str, root_path: str, data_path: str) -> tuple:
    """
    å¯¼å…¥ JSONL æ ¼å¼æ•°æ®é›†ï¼š
      1. è¯»å– JSONL æ–‡ä»¶
      2. æ ¡éªŒæ–‡ä»¶éç©º
      3. æ ¹æ®ç¬¬ä¸€æ¡è®°å½•æ¨æ–­æ•°æ®ç±»å‹
      4. å°† JSONL æ–‡ä»¶å¤åˆ¶åˆ° uploads ç›®å½•
      5. å°†å…ƒä¿¡æ¯å†™å…¥ SQLite
    è¿”å›å€¼: (æˆåŠŸæ ‡å¿—: bool, æç¤ºæ¶ˆæ¯: str)
    """
    if not os.path.isfile(data_path):
        return False, "æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„"

    # åˆ›å»ºæ•°æ®é›†ä¸“å±ç›®å½•
    dataset_dir = os.path.join(UPLOAD_DIR, dataset_name)
    os.makedirs(dataset_dir, exist_ok=True)

    # è¯»å–å¹¶è§£æ JSONL
    items = []
    with open(data_path, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f):
            try:
                item = json.loads(line.strip())
                items.append(item)
            except json.JSONDecodeError:
                return False, f"ç¬¬ {idx+1} è¡Œ JSON è§£æå¤±è´¥"

    if not items:
        return False, "æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œå¯¼å…¥å¤±è´¥"

    # æ¨æ–­æ•°æ®ç±»å‹
    data_type = get_data_type(items[0])

    # å¤åˆ¶åŸå§‹ JSONL æ–‡ä»¶
    new_data_filename = os.path.basename(data_path)
    new_data_path = os.path.join(dataset_dir, new_data_filename)
    shutil.copy2(data_path, new_data_path)

    # å†™å…¥æ•°æ®åº“ï¼Œä»…ä¿ç•™ content_pathï¼Œé¿å…å°†å¤§æ•°æ®å­˜å…¥ SQLite
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO datasets (name, path, upload_time, tags, data_type, content_path, root_path)"
        " VALUES (?, ?, ?, ?, ?, ?, ?)",
        (
            dataset_name,
            new_data_path,
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            '[]',
            data_type,
            new_data_path,
            root_path,
        )
    )
    conn.commit()

    return True, "æ•°æ®é›†å¯¼å…¥æˆåŠŸ"


def preview_dataset(dataset_id: int, page: int = 0) -> int:
    """
    åœ¨å‰ç«¯é¢„è§ˆæŒ‡å®šæ•°æ®é›†çš„å†…å®¹ã€‚
    æ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è§†é¢‘å±•ç¤ºï¼Œå¹¶æä¾›åˆ†é¡µåŠŸèƒ½ã€‚
    è¿”å›æ–°é¡µç : int
    """
    cursor = conn.cursor()
    cursor.execute(
        "SELECT content_path, data_type, root_path FROM datasets WHERE id = ?",
        (dataset_id,)
    )
    row = cursor.fetchone()
    if not row:
        st.error("æœªæ‰¾åˆ°å¯¹åº”æ•°æ®é›†")
        return page

    content_path, data_type, root_path = row
    with open(content_path, 'r', encoding='utf-8') as f:
        items = [json.loads(line) for line in f]

    start = page * ITEMS_PER_PAGE
    end = start + ITEMS_PER_PAGE

    # éå†å½“å‰é¡µæ•°æ®é¡¹
    for item in items[start:end]:
        st.markdown("---")
        st.write(f"**ID:** {item.get('id')}")
        for conv in item.get('conversations', []):
            prefix = "ğŸ‘¤ User:" if conv['from']=='human' else "ğŸ¤– Assistant:"
            st.write(prefix, conv['value'])

        # æ¸²æŸ“å›¾ç‰‡
        for idx, img in enumerate(item.get('images', [])):
            abs_path = os.path.join(root_path, img)
            if os.path.exists(abs_path):
                st.image(abs_path, caption=f"å›¾ç‰‡ {idx+1}")

        # æ¸²æŸ“è§†é¢‘
        for vid in item.get('video', []):
            abs_path = os.path.join(root_path, vid)
            if os.path.exists(abs_path):
                st.video(abs_path)

    # åˆ†é¡µæŒ‰é’®
    cols = st.columns(2)
    if page > 0 and cols[0].button("ä¸Šä¸€é¡µ", key=f"prev_{dataset_id}"):
        return page - 1
    if end < len(items) and cols[1].button("ä¸‹ä¸€é¡µ", key=f"next_{dataset_id}"):
        return page + 1

    return page

# ---------------------------------------------------------------------------- #
# Streamlit é¡µé¢å¸ƒå±€
# ---------------------------------------------------------------------------- #
st.set_page_config(page_title="å¤šæ¨¡æ€æ•°æ®ç®¡ç†å¹³å°", layout="wide")
st.title("å¤šæ¨¡æ€æ•°æ®ç®¡ç†å¹³å°")

# ä¸»å¯¼èˆª: æ•°æ®é›†åˆ—è¡¨ / å¯¼å…¥æ•°æ®é›†
tabs = st.tabs(["æ•°æ®é›†åˆ—è¡¨", "å¯¼å…¥æ•°æ®é›†"])

with tabs[0]:
    st.header("æ‰€æœ‰æ•°æ®é›†")
    datasets = load_all_datasets()
    if not datasets:
        st.info("å½“å‰å°šæ— æ•°æ®é›†ï¼Œè¯·å‰å¾€â€œå¯¼å…¥æ•°æ®é›†â€é¡µé¢æ·»åŠ ã€‚")
    else:
        for ds in datasets:
            ds_id, name, path, upload_time, tags_json, data_type, content_path, root_path = ds
            tags = json.loads(tags_json)

            with st.expander(f"{name} (ä¸Šä¼ : {upload_time}, ç±»å‹: {data_type}, æ•°é‡: {sum(1 for _ in open(content_path))} æ¡)"):
                # æ ‡ç­¾ç®¡ç†
                st.write("**æ ‡ç­¾:**", ", ".join([f"#{t}" for t in tags]) or "æ— ")
                new_tag = st.text_input("æ·»åŠ æ ‡ç­¾å¹¶å›è½¦", key=f"tag_in_{ds_id}")
                if new_tag:
                    if new_tag not in tags:
                        tags.append(new_tag)
                        update_tags(ds_id, tags)
                        st.experimental_rerun()

                # åˆ é™¤æ ‡ç­¾
                for t in tags:
                    if st.button(f"åˆ é™¤ #{t}", key=f"del_{ds_id}_{t}"):
                        tags.remove(t)
                        update_tags(ds_id, tags)
                        st.experimental_rerun()

                # é¢„è§ˆ
                if st.button("é¢„è§ˆæ•°æ®", key=f"view_{ds_id}"):
                    st.session_state[f"page_{ds_id}"] = 0
                if f"page_{ds_id}" in st.session_state:
                    cur_page = st.session_state[f"page_{ds_id}"]
                    new_page = preview_dataset(ds_id, cur_page)
                    if new_page != cur_page:
                        st.session_state[f"page_{ds_id}"] = new_page
                        st.experimental_rerun()

with tabs[1]:
    st.header("å¯¼å…¥æ–°æ•°æ®é›†")
    ds_name = st.text_input("æ•°æ®é›†åç§°", help="è‡ªå®šä¹‰å”¯ä¸€åç§°ï¼Œç”¨äºåŒºåˆ†ä¸åŒæ•°æ®é›†")
    root_path = st.text_input("æ ¹ç›®å½•è·¯å¾„", help="å›¾ç‰‡å’Œè§†é¢‘çš„æ ¹ç›®å½•ç»å¯¹è·¯å¾„")
    data_path = st.text_input("JSONL æ–‡ä»¶è·¯å¾„", help="ç¬¦åˆæ ¼å¼è¦æ±‚çš„ .jsonl æ–‡ä»¶ç»å¯¹è·¯å¾„")

    if st.button("å¼€å§‹å¯¼å…¥"):
        if not all([ds_name, root_path, data_path]):
            st.error("è¯·å¡«å†™å®Œæ•´çš„ä¿¡æ¯åå†å¯¼å…¥")
        else:
            ok, msg = import_jsonl_dataset(ds_name, root_path, data_path)
            if ok:
                st.success(msg)
                st.experimental_rerun()
            else:
                st.error(msg)
